{
  "models": [
    {
      "id": "google/gemma-2b-it",
      "name": "Gemma 2B-IT",
      "size_category": "medium",
      "parameters": "2B",
      "storage_size": "5.2GB",
      "supports_lora": true,
      "requirements": {
        "min_gpu_memory": {
          "standard": "16GB",
          "lora": "8GB",
          "qlora": "4GB"
        },
        "recommended_batch_size": {
          "cuda": {
            "standard": 1,
            "lora": 2,
            "qlora": 4
          },
          "rocm": {
            "standard": 1,
            "lora": 2,
            "qlora": 4
          },
          "cpu": 1
        }
      }
    },
    {
      "id": "facebook/opt-350m",
      "name": "OPT 350M",
      "size": "medium",
      "supports_lora": true,
      "requirements": {
        "min_gpu_memory": "8GB",
        "recommended_batch_size": {
          "cuda": 2,
          "rocm": 2,
          "cpu": 1
        }
      }
    },
    {
      "id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B",
      "name": "DeepSeek-R1-Distill-Qwen-14B",
      "size": "large",
      "supports_lora": true,
      "requirements": {
        "min_gpu_memory": {
          "standard": "28GB",
          "lora": "14GB",
          "qlora": "7GB"
        },
        "recommended_batch_size": {
          "cuda": {
            "standard": 1,
            "lora": 2,
            "qlora": 4
          },
          "rocm": {
            "standard": 1,
            "lora": 2,
            "qlora": 4
          },
          "cpu": 1
        }
      }
    },
    {
      "id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
      "name": "DeepSeek-R1-Distill-Qwen-1.5B",
      "size": "small",
      "supports_lora": true,
      "requirements": {
        "min_gpu_memory": {
          "standard": "4GB",
          "lora": "2GB",
          "qlora": "1GB"
        },
        "recommended_batch_size": {
          "cuda": {
            "standard": 4,
            "lora": 8,
            "qlora": 16
          },
          "rocm": {
            "standard": 4,
            "lora": 8,
            "qlora": 16
          },
          "cpu": 1
        }
      }
    },
    {
      "id": "microsoft/phi-4",
      "name": "Phi 4",
      "size_category": "medium",
      "parameters": "14.7B",
      "storage_size": "29.3GB",
      "supports_lora": true,
      "requirements": {
        "min_gpu_memory": {
          "standard": "16GB",
          "lora": "8GB",
          "qlora": "4GB"
        },
        "recommended_batch_size": {
          "cuda": {
            "standard": 1,
            "lora": 2,
            "qlora": 4
          },
          "rocm": {
            "standard": 1,
            "lora": 2,
            "qlora": 4
          },
          "cpu": 1
        }
      },
      "custom": true
    },
    {
      "id": "openai-community/gpt2",
      "name": "GPT 2",
      "size_category": "small",
      "parameters": "137.0M",
      "storage_size": "13.6GB",
      "description": "No description available",
      "supports_lora": true,
      "requirements": {
        "min_gpu_memory": {
          "standard": "16GB",
          "lora": "8GB",
          "qlora": "4GB"
        },
        "recommended_batch_size": {
          "cuda": {
            "standard": 1,
            "lora": 2,
            "qlora": 4
          },
          "rocm": {
            "standard": 1,
            "lora": 2,
            "qlora": 4
          },
          "cpu": 1
        }
      },
      "custom": true
    }
  ],
  "supported_backends": {
    "cuda": "NVIDIA CUDA",
    "rocm": "AMD ROCm",
    "cpu": "CPU Only"
  },
  "training_methods": {
    "standard": {
      "name": "Full Fine-tuning",
      "description": "Traditional fine-tuning of all model parameters",
      "memory_requirement": "high"
    },
    "lora": {
      "name": "LoRA",
      "description": "Low-Rank Adaptation for efficient fine-tuning",
      "memory_requirement": "medium"
    },
    "qlora": {
      "name": "QLoRA",
      "description": "Quantized Low-Rank Adaptation for very efficient fine-tuning",
      "memory_requirement": "low"
    }
  }
}